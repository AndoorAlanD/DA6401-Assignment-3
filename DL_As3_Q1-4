{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11837486,"sourceType":"datasetVersion","datasetId":7437128}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport random\nimport wandb\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:22.177933Z","iopub.execute_input":"2025-05-18T18:12:22.178693Z","iopub.status.idle":"2025-05-18T18:12:30.349251Z","shell.execute_reply.started":"2025-05-18T18:12:22.178663Z","shell.execute_reply":"2025-05-18T18:12:30.348360Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import wandb\nimport os\n\nos.environ['WANDB_API_KEY'] = '1ffc33d77af0fd022201ec32b81cd0e92cd75821'\nwandb.login()\n\n#1ffc33d77af0fd022201ec32b81cd0e92cd75821","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:30.351049Z","iopub.execute_input":"2025-05-18T18:12:30.351524Z","iopub.status.idle":"2025-05-18T18:12:36.607106Z","shell.execute_reply.started":"2025-05-18T18:12:30.351503Z","shell.execute_reply":"2025-05-18T18:12:36.606505Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malandandoor\u001b[0m (\u001b[33malandandoor-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"SOW_token = 0\nEOW_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.n_letters = 2 # Count SOW and EOW\n        self.letter2index = {}\n        self.letter2count = {}\n        self.index2letter = {0: \"0\", 1: \"1\"}\n\n    def addWord(self, word):\n        for ch in word:\n            self.addLetter(ch)\n\n    def addLetter(self, ch):\n        if ch not in self.letter2index:\n            self.letter2index[ch] = self.n_letters\n            self.letter2count[ch] = 1\n            self.index2letter[self.n_letters] = ch\n            self.n_letters += 1\n        else:\n            self.letter2count[ch] += 1\n\ninput_lang = Lang('eng')\noutput_lang = Lang('mal')\nx_train = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.train.tsv', sep='\\t', header=None) #, nrows=1000)\nx_val = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.dev.tsv', sep='\\t', header=None)\nx_test = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.test.tsv', sep='\\t', header=None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.607909Z","iopub.execute_input":"2025-05-18T18:12:36.608709Z","iopub.status.idle":"2025-05-18T18:12:36.790276Z","shell.execute_reply.started":"2025-05-18T18:12:36.608689Z","shell.execute_reply":"2025-05-18T18:12:36.789735Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MAX_LENGTH = 50\n\ndef indexesFromWord(lang, word):\n    return [lang.letter2index[ch] for ch in word]\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef wordFromTensor(lang, tensor):\n    s = \"\"\n    for i in tensor:\n        if(i.item()==1):\n            break\n        s += lang.index2letter[i.item()] \n    return s\n\ndef get_dataloader(x, input_lang, output_lang, batch_size):\n    pairs = list(zip(x[1].values, x[0].values))  # Get list of (input, target) tuples\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for i, (inp, tgt) in enumerate(pairs):\n        if not isinstance(inp, str) or not isinstance(tgt, str):\n            continue  # skip malformed entries\n\n        input_lang.addWord(inp)\n        output_lang.addWord(tgt)\n        inp_ids = indexesFromWord(input_lang, inp)\n        tgt_ids = indexesFromWord(output_lang, tgt)\n        inp_ids.append(EOW_token)\n        tgt_ids.append(EOW_token)\n        input_ids[i, :len(inp_ids)] = inp_ids\n        target_ids[i, :len(tgt_ids)] = tgt_ids\n\n    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                         torch.LongTensor(target_ids).to(device))\n    sampler = RandomSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.790930Z","iopub.execute_input":"2025-05-18T18:12:36.791129Z","iopub.status.idle":"2025-05-18T18:12:36.799380Z","shell.execute_reply.started":"2025-05-18T18:12:36.791112Z","shell.execute_reply":"2025-05-18T18:12:36.798588Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, config, input_size):\n        super(EncoderRNN, self).__init__()\n        \n        self.bidirectional = False\n            \n        self.embedding = nn.Embedding(input_size, config.inp_embed_size)\n        self.algo = algorithms[config.cell_type](config.inp_embed_size, config.hidden_size, config.num_enc, bidirectional = self.bidirectional, batch_first=True) #config.num_layers\n        self.dropout = nn.Dropout(config.dropout)\n        \n    def forward(self, input):\n        output, hidden = self.algo(self.dropout(self.embedding(input)))\n        return output, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.801117Z","iopub.execute_input":"2025-05-18T18:12:36.801927Z","iopub.status.idle":"2025-05-18T18:12:36.818780Z","shell.execute_reply.started":"2025-05-18T18:12:36.801901Z","shell.execute_reply":"2025-05-18T18:12:36.818075Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, config, output_size):\n        super(DecoderRNN, self).__init__()\n\n        self.out = nn.Linear(config.hidden_size, output_size)\n        self.config = config\n        self.bidirectional = False\n           \n        self.embedding = nn.Embedding(output_size, config.hidden_size)\n        self.algo = algorithms[config.cell_type](config.hidden_size, config.hidden_size, config.num_enc, bidirectional = self.bidirectional, batch_first=True) #config.num_layers\n        self.out = nn.Linear(config.hidden_size, output_size)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOW_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n    def forward_step(self, input, hidden):\n        output = F.relu(self.embedding(input))\n        output, hidden = self.algo(output, hidden)\n        output = self.out(output)\n        return output, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.819575Z","iopub.execute_input":"2025-05-18T18:12:36.820060Z","iopub.status.idle":"2025-05-18T18:12:36.837134Z","shell.execute_reply.started":"2025-05-18T18:12:36.820032Z","shell.execute_reply":"2025-05-18T18:12:36.836617Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n                decoder_optimizer, criterion, batch_size, teacher_forcing=True):\n\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for data in dataloader:\n        input_tensor, target_tensor = data  # shape: (B, MAX_LENGTH)\n        current_batch_size = input_tensor.size(0)\n\n        target_tensor2 = target_tensor if teacher_forcing else None\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor2)\n\n        outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))  # shape: (B*MAX_LENGTH, vocab_size)\n        labels = target_tensor.view(-1)  # shape: (B*MAX_LENGTH)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        total_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)  # shape: (B*MAX_LENGTH)\n\n        # Accuracy: Count how many complete sequences match\n        predicted = predicted.view(current_batch_size, MAX_LENGTH)\n        labels = labels.view(current_batch_size, MAX_LENGTH)\n\n        matches = (predicted == labels).all(dim=1)  # shape: (B,)\n        correct += matches.sum().item()\n        total += current_batch_size\n\n    # print(correct)\n\n    return total_loss / len(dataloader), (correct*100) / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.837977Z","iopub.execute_input":"2025-05-18T18:12:36.838188Z","iopub.status.idle":"2025-05-18T18:12:36.855161Z","shell.execute_reply.started":"2025-05-18T18:12:36.838173Z","shell.execute_reply":"2025-05-18T18:12:36.854512Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, n_epochs, config):\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.lr)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        print(\"Epoch:\",epoch)\n        loss, acc = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size)\n        print(\"Train: accuracy:\", acc, \"loss:\", loss)\n        if(acc<0.01 and epoch>=15):\n            break\n        val_loss, val_acc = train_epoch(val_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n        print(\"Validation: accuracy:\", val_acc, \"Loss:\", val_loss)\n        wandb.log({'train_accuracy': acc,'train_loss': loss,'val_accuracy': val_acc,'val_loss': val_loss})\n        \n    test_loss, test_acc = train_epoch(test_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n    print(\"Test: accuracy:\", test_acc, \"Loss:\", test_loss, \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.855755Z","iopub.execute_input":"2025-05-18T18:12:36.855923Z","iopub.status.idle":"2025-05-18T18:12:36.879116Z","shell.execute_reply.started":"2025-05-18T18:12:36.855908Z","shell.execute_reply":"2025-05-18T18:12:36.878474Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate(encoder, decoder):\n    for i in range(10):\n        print('Inp:', x_test[1][i])\n        print('Tag:', x_test[0][i])\n        output = ''\n        \n        with torch.no_grad():\n            input_tensor = tensorFromWord(input_lang, x_test[1][i])\n\n            encoder_outputs, encoder_hidden = encoder(input_tensor)\n            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n\n            _, topi = decoder_outputs.topk(1)\n            decoded_ids = topi.squeeze()\n\n            decoded_word = ''\n            for idx in decoded_ids:\n                if idx.item() == EOW_token:\n                    decoded_word+='1'\n                    break\n                decoded_word += output_lang.index2letter[idx.item()]\n            print('Out', decoded_word[:-1])\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.879893Z","iopub.execute_input":"2025-05-18T18:12:36.880138Z","iopub.status.idle":"2025-05-18T18:12:36.896659Z","shell.execute_reply.started":"2025-05-18T18:12:36.880115Z","shell.execute_reply":"2025-05-18T18:12:36.896086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_epochs = 20\n\nalgorithms = {'rnn': nn.RNN,'gru': nn.GRU,'lstm': nn.LSTM}\n\nsweep_config = {\n    'method': 'bayes', \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'inp_embed_size':{\n            'values': [64, 128, 256]\n        },\n        'num_dec': {\n            'values': [1, 2, 3]\n        },\n        'num_enc': {\n            'values': [1, 2, 3]\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n        'lr': {\n            'values': [0.001, 0.0001]\n        },\n        'hidden_size': {\n            'values': [256]\n        },\n        'batch_size': {\n            'values': [64, 128, 256]\n        },\n        'cell_type':{\n            'values': ['rnn', 'gru', 'lstm']\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='DL_A3')\n# sweep_id = \"cojvqj9\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:13:00.179395Z","iopub.execute_input":"2025-05-18T18:13:00.179677Z","iopub.status.idle":"2025-05-18T18:13:00.520904Z","shell.execute_reply.started":"2025-05-18T18:13:00.179658Z","shell.execute_reply":"2025-05-18T18:13:00.520302Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: jr6r1qrz\nSweep URL: https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/jr6r1qrz\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"'''\nBest Model\n'''\n# num_epochs = 50\n\n# algorithms = {'rnn': nn.RNN,'gru': nn.GRU,'lstm': nn.LSTM}\n\n# best_config = {\n#     'method': 'bayes', \n#     'metric': {\n#       'name': 'val_accuracy',\n#       'goal': 'maximize'   \n#     },\n#     'parameters': {\n#         'inp_embed_size':{\n#             'values': [256]\n#         },\n#         'num_dec': {\n#             'values': [1]\n#         },\n#         'num_enc': {\n#             'values': [2]\n#         },\n#         'dropout': {\n#             'values': [0.4]\n#         },\n#         'lr': {\n#             'values': [0.001]\n#         },\n#         'hidden_size': {\n#             'values': [64]\n#         },\n#         'batch_size': {\n#             'values': [64]\n#         },\n#         'cell_type':{\n#             'values': ['rnn']\n#         }\n#     }\n# }\n\n# sweep_id = wandb.sweep(sweep=best_config, project='DL_A3')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:12:36.913400Z","iopub.execute_input":"2025-05-18T18:12:36.914104Z","iopub.status.idle":"2025-05-18T18:12:36.933785Z","shell.execute_reply.started":"2025-05-18T18:12:36.914085Z","shell.execute_reply":"2025-05-18T18:12:36.933210Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\nBest Model\\n'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def test():\n    with wandb.init() as run:\n        config = wandb.config\n\n        # Run name formatting\n        wandb.run.name = (\n            f\"{config.cell_type}-E_{config.num_enc}-D_{config.num_enc}-\"\n            f\"do_{config.dropout}-bs_{config.batch_size}-lr_{config.lr}-\"\n            f\"hs_{config.hidden_size}-emb_{config.inp_embed_size}-\")\n            # f\"bid_{config.bidirectional}\")\n        \n        train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n        val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n        test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n        encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n        decoder = DecoderRNN(wandb.config, output_lang.n_letters).to(device)\n        print(input_lang.n_letters, output_lang.n_letters)\n        train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, num_epochs, wandb.config)\n        # encoder.eval()\n        # decoder.eval()\n        # evaluate(encoder, decoder)\n        \nwandb.agent('cojvqj9b', function=test) # calls main function for count number of times. , count=1\nwandb.finish() #cojvqj9b sweep_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:13:05.908991Z","iopub.execute_input":"2025-05-18T18:13:05.909242Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: erobol18 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinp_embed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_181312-erobol18</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/alandandoor-iit-madras/DL_A3/runs/erobol18' target=\"_blank\">cerulean-sweep-11</a></strong> to <a href='https://wandb.ai/alandandoor-iit-madras/DL_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/cojvqj9b' target=\"_blank\">https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/cojvqj9b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/alandandoor-iit-madras/DL_A3' target=\"_blank\">https://wandb.ai/alandandoor-iit-madras/DL_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/cojvqj9b' target=\"_blank\">https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/cojvqj9b</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/alandandoor-iit-madras/DL_A3/runs/erobol18' target=\"_blank\">https://wandb.ai/alandandoor-iit-madras/DL_A3/runs/erobol18</a>"},"metadata":{}},{"name":"stdout","text":"28 72\nEpoch: 1\nTrain: accuracy: 0.01884142372649104 loss: 0.5232642176770171\nValidation: accuracy: 0.08863676653075696 Loss: 0.5617788008759531\nEpoch: 2\nTrain: accuracy: 7.69243945051557 loss: 0.20905982219866046\nValidation: accuracy: 4.662293919517816 Loss: 0.3811968056003699\nEpoch: 3\nTrain: accuracy: 26.878147374190675 loss: 0.10573852697932054\nValidation: accuracy: 12.001418188264491 Loss: 0.31199466345015536\nEpoch: 4\nTrain: accuracy: 41.09999657428659 loss: 0.07045100035649793\nValidation: accuracy: 22.15919163268924 Loss: 0.2546836270040341\nEpoch: 5\nTrain: accuracy: 50.78448836970299 loss: 0.05192738120226845\nValidation: accuracy: 31.129232405601844 Loss: 0.21852867849422306\nEpoch: 6\nTrain: accuracy: 58.69959919153164 loss: 0.04025608596778582\nValidation: accuracy: 40.43609289133133 Loss: 0.18267000207070555\nEpoch: 7\n","output_type":"stream"}],"execution_count":null}]}