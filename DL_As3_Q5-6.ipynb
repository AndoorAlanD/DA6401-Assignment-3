{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11866425,"sourceType":"datasetVersion","datasetId":7456773},{"sourceId":11874952,"sourceType":"datasetVersion","datasetId":7462930}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport wandb\nimport torch\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nfrom torch import optim\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom prettytable import PrettyTable\nimport matplotlib.font_manager as fm\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\n# This used to print malayalam words when heatmap is ploted\nmalayalam_font = fm.FontProperties(fname='/kaggle/input/malayalam-font/Noto_Serif_Malayalam/NotoSerifMalayalam-VariableFont_wght.ttf')\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.182541Z","iopub.execute_input":"2025-05-19T18:23:16.182847Z","iopub.status.idle":"2025-05-19T18:23:16.188554Z","shell.execute_reply.started":"2025-05-19T18:23:16.182828Z","shell.execute_reply":"2025-05-19T18:23:16.188020Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"os.environ['WANDB_API_KEY'] = '1ffc33d77af0fd022201ec32b81cd0e92cd75821'\nwandb.login()\n\n#1ffc33d77af0fd022201ec32b81cd0e92cd75821","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.200012Z","iopub.execute_input":"2025-05-19T18:23:16.200293Z","iopub.status.idle":"2025-05-19T18:23:16.221574Z","shell.execute_reply.started":"2025-05-19T18:23:16.200276Z","shell.execute_reply":"2025-05-19T18:23:16.221065Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"SOW_token = 0\nEOW_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.n_letters = 2 # Count SOW and EOW\n        self.letter2index = {}\n        self.letter2count = {}\n        self.index2letter = {0: \"0\", 1: \"1\"}\n\n    def addWord(self, word):\n        for ch in word:\n            self.addLetter(ch)\n\n    def addLetter(self, ch):\n        if ch not in self.letter2index:\n            self.letter2index[ch] = self.n_letters\n            self.letter2count[ch] = 1\n            self.index2letter[self.n_letters] = ch\n            self.n_letters += 1\n        else:\n            self.letter2count[ch] += 1\n\ninput_lang = Lang('eng')\noutput_lang = Lang('mal')\nx_train = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.train.tsv', sep='\\t', header=None) #, nrows=1000)\nx_val = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.dev.tsv', sep='\\t', header=None)\nx_test = pd.read_csv('/kaggle/input/malayalam/ml/lexicons/ml.translit.sampled.test.tsv', sep='\\t', header=None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.222650Z","iopub.execute_input":"2025-05-19T18:23:16.222897Z","iopub.status.idle":"2025-05-19T18:23:16.334882Z","shell.execute_reply.started":"2025-05-19T18:23:16.222882Z","shell.execute_reply":"2025-05-19T18:23:16.334074Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"MAX_LENGTH = 50\n\ndef indexesFromWord(lang, word):\n    return [lang.letter2index[ch] for ch in word]\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef wordFromTensor(lang, tensor):\n    s = \"\"\n    for i in tensor:\n        if(i.item()==1):\n            break\n        s += lang.index2letter[i.item()] \n    return s\n\ndef get_dataloader(x, input_lang, output_lang, batch_size):\n    pairs = list(zip(x[1].values, x[0].values))  # Get list of (input, target) tuples\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for i, (inp, tgt) in enumerate(pairs):\n        if not isinstance(inp, str) or not isinstance(tgt, str):\n            continue  # skip malformed entries\n\n        input_lang.addWord(inp)\n        output_lang.addWord(tgt)\n        inp_ids = indexesFromWord(input_lang, inp)\n        tgt_ids = indexesFromWord(output_lang, tgt)\n        inp_ids.append(EOW_token)\n        tgt_ids.append(EOW_token)\n        input_ids[i, :len(inp_ids)] = inp_ids\n        target_ids[i, :len(tgt_ids)] = tgt_ids\n\n    input_lengths = (input_ids != 0).astype(np.uint8)\n    input_lengths = torch.BoolTensor(input_lengths).to(device)\n    \n    data = TensorDataset(torch.LongTensor(input_ids).to(device),torch.LongTensor(target_ids).to(device),input_lengths)\n\n    sampler = RandomSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.335775Z","iopub.execute_input":"2025-05-19T18:23:16.336135Z","iopub.status.idle":"2025-05-19T18:23:16.345212Z","shell.execute_reply.started":"2025-05-19T18:23:16.336103Z","shell.execute_reply":"2025-05-19T18:23:16.344679Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, config, input_size):\n        super(EncoderRNN, self).__init__()\n        \n        self.embedding = nn.Embedding(input_size, config.inp_embed_size)\n        self.algo = algorithms[config.cell_type](config.inp_embed_size, config.hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(config.dropout)\n        \n    def forward(self, input):\n        output, hidden = self.algo(self.dropout(self.embedding(input)))\n        return output, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.346582Z","iopub.execute_input":"2025-05-19T18:23:16.346798Z","iopub.status.idle":"2025-05-19T18:23:16.368436Z","shell.execute_reply.started":"2025-05-19T18:23:16.346781Z","shell.execute_reply":"2025-05-19T18:23:16.367891Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys, mask=None):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))  # (B, T, 1)\n        scores = scores.squeeze(2).unsqueeze(1)  # (B, 1, T)\n    \n        if mask is not None:\n            scores = scores.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n    \n        weights = F.softmax(scores, dim=-1)  # (B, 1, T)\n        context = torch.bmm(weights, keys)   # (B, 1, H)\n        return context, weights\n\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, config, output_size):\n        super(AttnDecoderRNN, self).__init__()\n        \n        self.dropout_p = config.dropout\n        hidden_size = config.hidden_size\n        \n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.attention = Attention(hidden_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.algo = algorithms[config.cell_type](hidden_size + hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, src_mask=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOW_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n        attentions = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs) # , src_mask\n            decoder_outputs.append(decoder_output)\n            attentions.append(attn_weights)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        attentions = torch.cat(attentions, dim=1)\n\n        return decoder_outputs, decoder_hidden, attentions\n\n\n    def forward_step(self, input, hidden, encoder_outputs):\n        embedded =  self.dropout(self.embedding(input))\n\n        # Ensure `embedded` has shape [B, 1, H]\n        if embedded.dim() == 2:\n            embedded = embedded.unsqueeze(1)  # [B, 1, H]\n        elif embedded.dim() == 4:\n            embedded = embedded.squeeze(1)  # Remove accidental extra dim\n\n\n         # Unpack hidden state if using LSTM\n        if isinstance(hidden, tuple):\n            h = hidden[0]  # (num_layers, batch, hidden_size)\n        else:\n            h = hidden\n    \n        query = h.permute(1, 0, 2)  # (batch, num_layers, hidden_size)\n\n        context, attn_weights = self.attention(query, encoder_outputs)\n        # Ensure `context` has shape [B, 1, H]\n        if context.dim() == 2:\n            context = context.unsqueeze(1)\n        elif context.dim() == 4:\n            context = context.squeeze(1)\n    \n        input_gru = torch.cat((embedded, context), dim=2)\n\n        output, hidden = self.algo(input_gru, hidden)\n        output = self.out(output)\n\n        return output, hidden, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.369311Z","iopub.execute_input":"2025-05-19T18:23:16.370168Z","iopub.status.idle":"2025-05-19T18:23:16.399187Z","shell.execute_reply.started":"2025-05-19T18:23:16.370135Z","shell.execute_reply":"2025-05-19T18:23:16.398457Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n          decoder_optimizer, criterion, batch_size, teacher_forcing = True):\n\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for data in dataloader:\n        input_tensor, target_tensor, src_mask = data \n        # input_tensor, target_tensor = data\n\n        target_tensor2 = None\n        if (teacher_forcing):\n            target_tensor2 = target_tensor\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        \n        decoder_outputs, _, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor, src_mask)\n        # decoder_outputs, _, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor2)\n\n        outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))\n        labels = target_tensor.view(-1)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n\n        seq_len = target_tensor.size(1)  # Typically MAX_LENGTH\n        actual_batch_size = target_tensor.size(0)\n        flat_len = predicted.size(0)\n        \n        for i in range(actual_batch_size):\n            start = i * seq_len\n            end = start + seq_len\n            if end > flat_len:\n                break  # Avoid out-of-bounds\n            if torch.equal(predicted[start:end], labels[start:end]):\n                correct += 1\n        total += actual_batch_size\n\n    return total_loss / len(dataloader), (correct*100) / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.399997Z","iopub.execute_input":"2025-05-19T18:23:16.400241Z","iopub.status.idle":"2025-05-19T18:23:16.427196Z","shell.execute_reply.started":"2025-05-19T18:23:16.400226Z","shell.execute_reply":"2025-05-19T18:23:16.426659Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, n_epochs, config):\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.lr)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        print(\"Epoch:\",epoch)\n        loss, acc = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size)\n        print(\"Train: accuracy:\", acc, \"loss:\", loss)\n        if(acc<0.01 and epoch>=15):\n            break\n        val_loss, val_acc = train_epoch(val_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n        print(\"Validation: accuracy:\", val_acc, \"Loss:\", val_loss)\n        wandb.log({'train_accuracy': acc,'train_loss': loss,'val_accuracy': val_acc,'val_loss': val_loss})\n        \n    test_loss, test_acc = train_epoch(test_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n    print(\"Test: accuracy:\", test_acc, \"Loss:\", test_loss, \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.429198Z","iopub.execute_input":"2025-05-19T18:23:16.429369Z","iopub.status.idle":"2025-05-19T18:23:16.454951Z","shell.execute_reply.started":"2025-05-19T18:23:16.429356Z","shell.execute_reply":"2025-05-19T18:23:16.454267Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"num_epochs = 20\n\nalgorithms = {'rnn': nn.RNN,'gru': nn.GRU,'lstm': nn.LSTM}\n\nsweep_config = {\n    'method': 'bayes', \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'inp_embed_size':{\n            'values': [64, 128, 256]\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n        'lr': {\n            'values': [0.001, 0.0001]\n        },\n        'hidden_size': {\n            'values': [128, 256]\n        },\n        'batch_size': {\n            'values': [64, 128, 256]\n        },\n        'cell_type':{\n            'values': ['rnn', 'gru', 'lstm']\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='DL_A3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.455700Z","iopub.execute_input":"2025-05-19T18:23:16.456342Z","iopub.status.idle":"2025-05-19T18:23:16.478391Z","shell.execute_reply.started":"2025-05-19T18:23:16.456321Z","shell.execute_reply":"2025-05-19T18:23:16.477864Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def test():\n    with wandb.init() as run:\n        config = wandb.config\n\n        # Run name formatting\n        wandb.run.name = (\n            f\"{config.cell_type}-do_{config.dropout}-bs_{config.batch_size}-lr_{config.lr}-\"\n            f\"hs_{config.hidden_size}-emb_{config.inp_embed_size}-\")\n        \n        train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n        val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n        test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n        encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n        decoder = AttnDecoderRNN(wandb.config, output_lang.n_letters).to(device)\n        print(input_lang.n_letters, output_lang.n_letters)\n        train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, num_epochs, wandb.config)\n        \nwandb.agent(sweep_id, function=test, count=1) # calls main function for count number of times. , count=1\nwandb.finish() #cojvqj9b sweep_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.479190Z","iopub.execute_input":"2025-05-19T18:23:16.479414Z","iopub.status.idle":"2025-05-19T18:23:16.501318Z","shell.execute_reply.started":"2025-05-19T18:23:16.479389Z","shell.execute_reply":"2025-05-19T18:23:16.500820Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def evaluate_and_save_predictions(encoder, decoder):\n    results = []\n    output_file = 'test_predictions.tsv'\n\n    with torch.no_grad():\n        for i in range(len(x_test[0])):\n            input_seq = x_test[1][i]\n            true_output = x_test[0][i]\n\n            input_tensor = tensorFromWord(input_lang, input_seq)\n            encoder_outputs, encoder_hidden = encoder(input_tensor)\n\n            decoder_input = torch.tensor([[SOW_token]], device=device)\n            decoder_hidden = encoder_hidden\n            decoded_ids = []\n            attentions = []\n\n            for _ in range(MAX_LENGTH):\n                decoder_output, decoder_hidden, attn_weights = decoder.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n                _, topi = decoder_output.topk(1)\n                next_token = topi.item()\n                if next_token == EOW_token:\n                    break\n                decoded_ids.append(next_token)\n                attentions.append(attn_weights.squeeze(1).cpu().numpy())\n                decoder_input = topi.detach()\n\n            predicted_output = ''.join(output_lang.index2letter.get(idx, '?') for idx in decoded_ids)\n\n            results.append((input_seq, true_output, predicted_output, attentions))\n\n    # Save predictions to TSV\n    df = pd.DataFrame([(x, y, z) for x, y, z, _ in results],\n                      columns=[\"Input\", \"Ground Truth\", \"Prediction\"])\n    df.to_csv(output_file, index=False, sep='\\t', encoding='utf-8-sig')\n    print(f\"\\n📁 All test predictions saved to: {output_file}\")\n    \n    # Plot grid of attention heatmaps for 9 random samples\n    print(f\"\\n🧠 Showing attention heatmaps for 9 random predictions:\")\n    show_attention_grid(random.sample(results, min(9, len(results))))\n\n\n\ndef show_attention_grid(samples):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    axes = axes.flatten()\n\n    for idx, (input_seq, true_output, predicted_output, attentions) in enumerate(samples):\n        ax = axes[idx]\n        input_chars = list(input_seq)\n        output_chars = list(predicted_output)\n        attn_matrix = np.array(attentions).squeeze(1)\n\n        sns.heatmap(\n            attn_matrix[:len(output_chars), :len(input_chars)],\n            xticklabels=input_chars,\n            yticklabels=output_chars,\n            cmap='viridis',\n            ax=ax,\n            cbar=False\n        )\n        ax.set_yticklabels(ax.get_yticklabels(), fontproperties=malayalam_font, rotation=0)\n        ax.set_title(f\"In: {input_seq}\\nGT: {true_output}\\nPred: {predicted_output}\", \n             fontsize=9, fontproperties=malayalam_font)\n        # ax.set_title(f\"In: {input_seq}\\nGT: {true_output}\\nPred: {predicted_output}\", fontsize=9)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n\n    # Hide any unused subplots\n    for j in range(len(samples), 9):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.543244Z","iopub.execute_input":"2025-05-19T18:23:16.543447Z","iopub.status.idle":"2025-05-19T18:23:16.559887Z","shell.execute_reply.started":"2025-05-19T18:23:16.543432Z","shell.execute_reply":"2025-05-19T18:23:16.559429Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Best Model\nnum_epochs = 1\n\nalgorithms = {'rnn': nn.RNN,'gru': nn.GRU,'lstm': nn.LSTM}\n\nbest_config = {\n    'method': 'bayes', \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'inp_embed_size':{\n            'values': [256]\n        },\n        'dropout': {\n            'values': [0.3]\n        },\n        'lr': {\n            'values': [0.001]\n        },\n        'hidden_size': {\n            'values': [256]\n        },\n        'batch_size': {\n            'values': [64]\n        },\n        'cell_type':{\n            'values': ['lstm']\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep=best_config, project='DL_A3')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:23:16.560560Z","iopub.execute_input":"2025-05-19T18:23:16.560809Z","iopub.status.idle":"2025-05-19T18:23:16.836267Z","shell.execute_reply.started":"2025-05-19T18:23:16.560788Z","shell.execute_reply":"2025-05-19T18:23:16.835541Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: lv2nh1c7\nSweep URL: https://wandb.ai/alandandoor-iit-madras/DL_A3/sweeps/lv2nh1c7\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def test():\n    with wandb.init() as run:\n        config = wandb.config\n\n        # Run name formatting\n        wandb.run.name = (\n            f\"{config.cell_type}-do_{config.dropout}-bs_{config.batch_size}-lr_{config.lr}-\"\n            f\"hs_{config.hidden_size}-emb_{config.inp_embed_size}-\")\n        \n        train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n        val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n        test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n        encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n        decoder = AttnDecoderRNN(wandb.config, output_lang.n_letters).to(device)\n        print(input_lang.n_letters, output_lang.n_letters)\n        train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, num_epochs, wandb.config)\n        encoder.eval()\n        decoder.eval()\n        evaluate_and_save_predictions(encoder, decoder)\n        \nwandb.agent(sweep_id, function=test, count=1) # calls main function for count number of times. , count=1\nwandb.finish() #cojvqj9b sweep_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}